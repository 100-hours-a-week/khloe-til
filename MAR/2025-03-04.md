# TIL Template

## 날짜: 2025-03-04

### [새로 배운 내용]
### 주제 : 딥러닝 II
#### Model Architecture
- “이 문제를 풀기위해 어떻게 모델을 구성해야할까?”
- 학습률이 너무 크면 최적의 손실 값을 찾기 어렵고, 너무 작아도 학습이 너무 오래걸리거나 지역 최적점(local minimum)에 빠질 수 있음
  - 1. Epoch를 한 번 돌 때마다 모델이 하나씩 생성되고 - 2. Epoch를 전부 다 돈 후에는 Test Data를 사용해서 테스트 진행(Epoch 수만큼 반복) - 3. 2번의 텥스트 과정에서 성능이 가장 좋은 모델이 최종 모델이 됨
#### Pre-trained Model
: 처음부터 구축하지 않아도 이미 훈련이 끝난 모델 - 시간과 자원이 절약되기 때문에 사용한다.
#### ResNet (Residual Network)
: 기울기 소실 문제를 해결하기 위해 Residual Connections(Skip Connections, 잔차 연결)을 도입한 사전 훈련 모델
#### VGG16
: 깊고 구조화된 컨볼루션 신경망으로, 16개의 가중치 층(13개의 컨볼루션 레이어와 3개의 풀리 커넥티드 레이어)으로 구성되며, 주로 이미지 인식과 분류 작업에 사용되는 사전 훈련된 모델
imageNet으로 사전 훈련된 가중치를 제공한다.
#### 전이 학습(Tranfer Learning)
: 사전 훈련된 모델을 그대로 사용하거나 추가 튜닝하여 새로운 문제(데이터셋)에 적용함으로써 시간을 절약하고 성능을 개선 시키는 머신러닝 기법
#### Fine Tuning
: 사전 훈련된 모델을 새로운 데이터셋에 맞춰 성능을 최적화하는 과정 - 학습 시간 단축, 데이터 부족 문제 극복 ...
#### Feature Extraction (특징 추출)
: 사전 훈련된 모델의 중간 계층에 이미 학습된 가중치와 필터는 고정(freeze)하여, 그 계층을 특징 추출기로 활용하는 전이 학습 기법
#### Overfitting (과적합) <-> Underfitting (과소적합)
Overfitting (과적합)  : 모델이 너무 지나치게 학습하여 일반화 성능이 떨어져 실제 데이터에서 오차가 증가하는 현상
Underfitting (과소적합) : 모델이 데이터의 복잡성을 충분히 학습하지 못해 성능 자체가 낮은 것

### 오늘의 도전 과제와 해결 방법
- 금일 복습 + 위클리 과제 + 미니 퀘스트 50% 달성

### 오늘의 회고
- 수업의 양이 많기도 하고, 파이썬을 처음 공부해서 그런지 복습이 많이 필요하다고 느껴진 하루
- 복습을 오래 했지만 다 끝내지 못함
- 다른 개인 공부를 못했기 때문에 속도를 더 높여서(몰입) 복습하고 개인 공부 시간도 채울 수 있도록 하기


### 참고 내용
- 앙상블(Ensemble) 기법: 여러 모델을 결합하여 예측 성능을 높임
- 여러 구조 중 장점만 합쳐서 좋은 모델이 나올 수 있음 → 많이 해봐라
- 비동기 처리
- 사용자 경험을 높이려면 어떻게 해야할까